---
title: "Can Data Predict Bergenâ€™s Bysykkel Usage? Insights and Trends from 2024"
author: "Syed Amjad Ali"
format:
  html:
    self-contained: true
    code-fold: true  # Allows code folding
    code-tools: true
    toc: true        # Adds a table of contents
    toc-location: left  # Optional: places TOC in the left sidebar
editor: visual
---

```{r setup, echo=FALSE, include=FALSE}
# Global options or configurations
knitr::opts_chunk$set(warning=FALSE, message=FALSE)
```

## Introduction

This project analyzes bicycle rental patterns across Bergen, Norway, using historical data from [Bergen Bysykkel](https://bergenbysykkel.no/apne-data/historisk) and hourly weather observations obtained from [SeKlima](https://seklima.met.no/observations/?fbclid=IwY2xjawHC3BlleHRuA2FlbQIxMAABHR-1J8AxQO7W68khnBEDM7aVue4GLeWghu0CrBYHs3b1dowE8Wq2u1oBXQ_aem_HFWzKgdTgLH6cHBCCTq4RA). This analysis is structured into two parts:

[Part 1: Exploratory Analysis](#sec-part-1-exploratory-analysis) focuses on foundational data cleaning, exploratory analysis, and simple predictive modelling.

[Part 2: Predictive Analytics](#sec-part-2-predictive-analytics) will integrate weather data and employ advanced predictive techniques to refine our understanding of rental trends.

This project aims to address key operational and strategic questions: - How does bicycle usage vary by station and time of day in Bergen? - How can we predict hourly ride counts to better manage station capacity? - What insights can visualizations provide for urban planning and resource allocation?

By tackling these questions and leveraging robust datasets, we aim to uncover actionable insights to enhance Bergen's bike-sharing system and improve user experience.

## Part 1: Exploratory Analysis {#sec-part-1-exploratory-analysis}

### Setup

#### Loading Required Libraries

```{r}
library(magrittr)
library(dplyr)
library(readr)
library(data.table)
library(assertthat)
library(rsample)
library(purrr)
library(tidyverse)
library(ggmap)
library(leaflet)
library(knitr)
```

### Data Collection and Preprocessing

#### Loading Initial Dataset

Since the bike ride data is downloaded in 12 separate `.csv` files (one for each month), we first need to combine them into a single dataset. This will allow us to perform a unified analysis of all bike rides recorded between January 1, 2024, and December 8, 2024.

```{r}
# Read in all bike rides from January 1, 2024, to December 8, 2024.
# Get a list of all `.csv` files in the "byssykkel-data-2024" directory
filenames <- list.files("byssykkel-data-2024", pattern=".csv", full.names=TRUE)
# Combine all the data from these files into a single dataset.
bike_rides_2024_data <- rbindlist(lapply(filenames,fread))

# Render a formatted table for the first 10 rows
kable(head(bike_rides_2024_data, 5), format = "html")
```

#### Creating Station-Date-Hour Combinations

To analyze bike rides at a granular level, we create a comprehensive combination of all stations, dates, and hours of the day. This ensures that every possible station-hour observation is included, even if no rides occurred during a specific hour.

```{r}
# Extract unique station IDs from the dataset to represent all bike stations.
start_station_id = as.double(unique(bike_rides_2024_data$start_station_id))

# Create a sequence of every hour from January 1, 2024, to December 19, 2024.

floor_start_dh <- seq(as.POSIXct("2024-01-01 00:00:00"), as.POSIXct("2024-12-19 23:00:00"), by = "hours")

# Combine all station IDs with every hour to create all possible station-hour combinations.
df <- 
  expand.grid(floor_start_dh, start_station_id) %>%
  rename(floor_start_dh = Var1,
         start_station_id = Var2) %>% 
  arrange(start_station_id)

# Preview the first 10 rows of the dataset.
head(df,10)

```

### Data Aggregation

#### Calculating Hourly Bicycle Ride Counts

In this step, we calculate the total number of bicycle rides for each station and each hour. This allows us to analyze hourly trends and activity levels at different stations across Bergen

```{r}
# Convert the "started_at" column to hourly timestamps (YYYY-MM-DD HH:00:00 format). 
bike_rides_2024 <- bike_rides_2024_data %>% 
  mutate(start_date = format(as.POSIXct(.$started_at, '%Y-%m-%d %H:%M:%S'), '%Y-%m-%d %H:%00:%00')) %>% 
  
# Group the data by station ID and hourly timestamps.
  group_by(start_station_id, start_date) %>%
  
 # Count the number of rides for each station-hour combination.
  summarise(n_rides = n())

# Convert the "start_date" column to a proper date-time format for easier analysis later.
bike_rides_2024$start_date <- as.POSIXct(bike_rides_2024$start_date )

# The result is a dataset with station IDs, hourly timestamps, and the corresponding ride counts.
bike_rides_2024
```

#### Handling Missing Values in the Dataset

In this step, we join the aggregated dataset of station-hour combinations with the hourly ride counts. Any missing values (indicating no rides during a specific hour at a station) are filled with `0` to ensure a complete dataset for analysis.

```{r}
# Join the station-hour combinations (df) with the hourly ride counts (bike_rides_2024).
# This ensures every possible station-hour combination is included, even if no rides occurred.
df_agg <- left_join(df, bike_rides_2024, by = c("start_station_id" = "start_station_id", "floor_start_dh" = "start_date"))

# Replace all missing values (NA) with 0, representing no rides for those specific hours and stations.
df_agg [is.na(df_agg )] <- 0
```

#### Adding Weekday and Hour Information

Here, we add two new features to the dataset: the day of the week (represented as numbers from 1 to 7) and the hour of the day (represented as numbers from 0 to 23). These features will help in identifying trends based on time, such as peak hours or weekday patterns.

```{r}

# Add two new columns to the dataset:
# - `start_hour`: Represents the hour of the day (0 to 23) for each observation.
# - `weekday_start`: Represents the day of the week (1 = Sunday, 7 = Saturday).
df_agg <- df_agg %>% 
  mutate(start_hour=as.factor(hour(floor_start_dh)),weekday_start=as.factor(wday(floor_start_dh)))

# Display the first 10 rows of the updated dataset for verification.
head(df_agg,10)
```

The dataset now contains 8760 observations per station (one per hour for the entire year), capturing total ride counts for each hour.

### Data Validation

#### Ensuring Data Integrity Across Stations and Hours

To ensure the accuracy of the dataset, we perform a validation test to confirm there are no duplicate records for any station-hour combination. This step is critical to maintaining the integrity of the data for analysis.

```{r}
# Validation Test 1: Check for duplicate records
# Group the data by station ID and hourly timestamp.
# Summarize the number of records for each station-hour combination.
# Ensure that the maximum count (`max_n`) is equal to 1, indicating no duplicates.
#Test 1
assert_that(df_agg %>%
              group_by(start_station_id, floor_start_dh) %>%
              summarise(n = n()) %>%
              ungroup() %>%
              summarise(max_n = max(n)) %$%
              max_n == 1,
         # Error message to display if duplicates are found.
            msg = "Duplicates on stations/hours/dates")
```

#### Verifying Time Intervals Between Observations

This validation step ensures that the time intervals between consecutive observations for each station are consistent (exactly one hour). Any irregularities in the time intervals could indicate missing or misaligned data, which needs to be addressed before analysis.

```{r}
#Verify that the time difference between consecutive observations is exactly 1 hour.
#Test 2
assert_that(df_agg %>%
              group_by(start_station_id) %>%
              mutate(
                timediff = floor_start_dh - lag(floor_start_dh,# Calculate the time difference with the previous observation.
                                                order_by = floor_start_dh) # Ensure the observations are ordered by timestamp.
              ) %>%
              
# Filter rows where the time difference is not equal to 1 hour.
              filter(as.numeric(timediff) != 1) %>%
  
# Count rows that violate the 1-hour interval rule. If none, this will return 0.
              nrow(.) == 0,
            msg="Time diffs. between obs are not always 1 hour")
```

### Exploratory Visualizations

#### Analyzing Hourly Bicycle Ride Counts by Station

To facilitate geospatial analysis, we calculate the average latitude and longitude for each station based on historical data. These coordinates allow us to map the stations and analyze bicycle ride counts geographically. We then merge this information into the aggregated dataset for further exploration.

```{r}

# Calculate average latitude and longitude for each station
# - Select relevant columns: station ID, longitude, and latitude.
# - Group by station ID to calculate the mean longitude and latitude across all observations.
station  <- bike_rides_2024_data %>% 
  select(start_station_id, start_station_longitude, start_station_latitude) %>% 
  group_by(start_station_id) %>% 
  summarise(lon = sum(start_station_longitude)/n(),# Average longitude for each station. 
            lat = sum(start_station_latitude)/n()# Average latitude for each station.
            ) 
station


# Merge average coordinates into the main dataset (df_agg)
# - Perform a left join to add longitude and latitude columns to `df_agg`.
df_agg_lonlat <- left_join(df_agg, station , by = c("start_station_id" = "start_station_id"))

# Display the first 10 rows of the updated dataset for verification.

kable(head(df_agg_lonlat,10), format = "html")
```

#### Identifying Weekly Ride Patterns

In this section, we leverage **Leaflet**, a popular open-source library for interactive mapping, to create a dynamic visualization of bicycle ride patterns across Bergen. Unlike some mapping tools that require API access keys and usage fees (e.g., Google Maps API), Leaflet uses free OpenStreetMap (OSM) tiles, making it an accessible choice for projects like this. By combining the geographical data of stations with ride counts, the interactive map provides a clear picture of traffic hotspots and trends. The map includes features like scalable markers, hover tooltips, and customizable legends for enhanced user experience.

```{r}


# Define a function to create an interactive map visualizing bicycle traffic volume
plot_map_leaflet <- function(date_and_hour) {
   # Filter the dataset for the specified date and hour
  df <- df_agg_lonlat %>%
    filter(floor_start_dh == date_and_hour)
  
  
  # Define color categories based on the number of rides
  ride_colors <- colorFactor(
    palette = c("#e41a1c", "#377eb8", "#4daf4a", "#984ea3", "#ff7f00", "#ffff33"), # Added a color for >4
    levels = c(0, 1, 2, 3, 4, "5+"),  # Add a category for "5+"
    na.color = "black"
  )
  
  # Add a new column to categorize the number of rides
  df <- df %>%
    mutate(
      ride_category = case_when(
        n_rides <= 4 ~ as.character(n_rides),# Convert rides <= 4 to strings
        n_rides > 4  ~ "5+"  # Group all rides >4 into "5+"
      )
    )
  
 # Create an interactive map using Leaflet  
  leaflet(df) %>%
    addTiles() %>%  # Add OpenStreetMap tiles as the base map
    addCircleMarkers(
      lng = ~lon,  # Longitude for marker placement
      lat = ~lat,  # Latitude for marker placement
      radius = ~ifelse(n_rides > 0, n_rides * 3, 5),  # Scaled marker size, minimum size for 0 rides
      color = ~ride_colors(ride_category),  # Apply distinct colors based on ride_category
      label = ~paste("Station ID:", start_station_id, "<br>",
                     "Rides:", n_rides),  # Tooltip for hover
      popup = ~paste("<strong>Station ID:</strong>", start_station_id, "<br>",
                     "<strong>Number of Rides:</strong>", n_rides, "<br>",
                     "<strong>Time:</strong>", date_and_hour)  # Detailed popup
    ) %>%
    
    addLegend(
      "bottomright",  # Place legend at the bottom right
      pal = ride_colors, # Use ride color categories
      values = c(0, 1, 2, 3, 4, "5+"), # Define legend categories
      title = paste("Rides on", date_and_hour),  # Dynamic title
      opacity = 1  # Set legend opacity
    ) %>%
    addScaleBar(position = "bottomleft") %>% # Add a scale bar for reference
    addMiniMap(toggleDisplay = TRUE) %>%  # Add a mini-map for navigation
    addControl(
      html = paste("<h4>Bicycle Traffic Volume at", date_and_hour, "</h4>"),# Add a custom title
      position = "topright"
    )
}

    


## Test the function with specific date and time inputs
plot_map_leaflet("2024-06-08 13:00:00")


##
plot_map_leaflet("2024-05-18 15:00:00")
```

### Basic Statistical Analysis

Linear regression is a statistical method that models the relationship between a dependent variable (response) and independent variables (predictors). The general form of a linear regression model can be expressed as:

$$
y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_k x_k + \epsilon
$$

Here:

-   $(y)$: Number of bike rides (response variable).
-   $(\beta_0)$: Intercept term, representing the baseline value of $(y)$ when all predictors are zero.
-   $(\beta_1, \beta_2, \dots, \beta_k)$: Coefficients representing the effect of predictors on $(y)$.
-   $(x_1, x_2, \dots, x_k)$: Predictors (e.g., weekday, hour of the day).
-   $(\epsilon)$: Residual error, capturing the variability in $(y)$ not explained by the predictors.

#### Data Splitting for Model Training

For this project, we split the dataset into \*\*training\*\* and \*\*testing\*\* sets for each station to ensure robust model development and evaluation. The training set is used to estimate the coefficients $(\beta_0, \beta_1, \dots, \beta_k)$, while the test set is used to validate how well the model generalizes to unseen data.

The data is divided such that:

$$
\text{Training Set Size} = 0.75 \times N, \quad \text{Testing Set Size} = 0.25 \times N
$$

Where $(N)$ is the total number of observations for a specific station. This ensures that the model is trained on a sufficient amount of data while retaining enough data for unbiased validation.

```{r}
# The data is grouped by station ID to ensure each station is processed independently.
# For each station, the data is split into training (75%) and testing (25%) sets.
by_station <- df_agg %>% 
  group_by(start_station_id) %>%# Grouping by station ID for independent splits 
  summarize(split = list(initial_time_split(cur_data()))) %>% # Splitting the data
  group_by(start_station_id) %>% 
  mutate(data.train=list(training(split[[1]])),# Extracting the training set
         data.test=list(testing(split[[1]]))) %>%# Extracting the testing set
  select(!split)# Removing the unnecessary split column
by_station # View the resulting grouped data structure with train/test sets



# Summarizing the first two training datasets to check the split.
map(setNames(by_station$data.train, by_station$start_station_id)[1:2], summary)

# Summarizing the first two testing datasets to ensure splits are correct.
map(setNames(by_station$data.test, by_station$start_station_id)[1:2], summary)
```

#### Station-Specific Linear Regression Models

In this section, we estimate station-specific linear regression models to predict the number of bike rides based on the day of the week and the hour of the day. Each station is modeled individually to capture its unique traffic patterns. The explanatory variables (`weekday_start` and `start_hour`) are treated as factors to account for their categorical nature.

#### Explanation of the Approach:

Linear regression models are fitted separately for each station using the `purrr::map` function, which efficiently applies the modeling function to the training data of all stations. The models for specific stations (e.g., Station 2 and Station 3) are also summarized to evaluate their coefficients and significance.

The regression equation for each station is:

```{r}

## A function that fits the linear regression model to each station
station_model <- function(df) {
  lm(n_rides~weekday_start + start_hour, data=df)
}
# put the linear model to each station by using purrr:map() to apply the model to each element
models <- map(by_station$data.train, station_model)
# Put the model inside a mutate
by_station <- by_station %>%
  group_by(start_station_id) %>%
  mutate(model = map(data.train, station_model)) %>% 
  ungroup()
by_station





## Check models of station 2 and station 3
by_station %>% 
  filter(start_station_id == 2) %>% 
  pluck("model", 1) %>% 
  summary()

by_station %>% 
  filter(start_station_id == 3) %>% 
  pluck("model", 1) %>% 
  summary()
```

The results of the station-specific models provide insights into how different days and times affect bike usage across Bergen. For example, coefficients of the `weekday_start` variable reveal the expected change in bike rides for different days compared to the baseline day (e.g., Sunday). Similarly, the `start_hour` coefficients highlight hourly variations in bike usage.

By modeling each station independently, we can better understand the localized trends and make more precise predictions.

#### Predicting Hourly Bicycle Rides

In this section, we use the station-specific linear regression models to predict the hourly bike rides for each station. These predictions are generated by applying the fitted models to the test data, enabling us to assess how well the models generalize to unseen data. By focusing on station-level predictions, we capture the unique characteristics and trends of each station, such as peak times and busy days.

The predictions are stored in a structured dataset that includes station IDs and their corresponding predicted ride counts for every hour. This consolidated dataset provides the foundation for analyzing trends and visualizing patterns across the entire city.

```{r}

# Generate predictions for each station using the test data


by_station <- by_station %>%
  group_by(start_station_id) %>%
  mutate(predictions = list(predict(model[[1]], newdata=data.test[[1]]))) %>%
  ungroup()
by_station


# Define a helper function to extract predictions and test data for a specific station
prediction_df_from_station_nr <- function(station_nr) {
  by_station %>%
    filter(start_station_id == station_nr) %>%
    ungroup() %>%
    select(c(data.test, predictions)) %>%
    unnest(c(data.test, predictions)) %>%
    mutate(start_station_id = station_nr)
}


# Combine predictions for all stations into a single dataset
predictions_from_all_stations <- bind_rows(map(by_station$start_station_id, prediction_df_from_station_nr))


predictions_from_all_stations # Display the consolidated predictions dataset
```

#### Visualizing Predicted Ride Patterns

In this section, we visualize the predicted hourly bicycle ride patterns for all stations over an entire week. The objective is to understand how ride volumes vary across different hours of the day and days of the week. This visualization provides insights into station-specific trends and helps identify periods of high and low activity.

The plot uses separate panels for each day of the week, with predicted ride volumes on the y-axis and the hour of the day on the x-axis. Each station is represented by a unique color, offering a comprehensive view of trends across all stations, even though detailed comparisons between individual stations maybe less emphasized due to the scale of the data.

```{r fig.height=12, fig.width=10}

# Filter data for the week of 2024-10-01 to 2024-10-07 and create the plot


predictions_from_all_stations %>%
  mutate(start_station_id = factor(start_station_id, 
                                   labels = str_wrap(levels(factor(start_station_id)), width = 10))) %>%
  filter(floor_start_dh > "2024-10-01 00:00:00", floor_start_dh <= "2024-10-08 00:00:00") %>%
  ggplot(aes(x = as.numeric(start_hour), y = predictions, color = start_station_id)) +
  geom_line() +
  labs(
    x = "Start Hour",
    y = "Predictions",
    colour = "Start Station ID",
    title = "Hourly Bicycle Ride Count Prediction Per Station from 2024-10-01 to 2024-10-07"
  ) + 
  facet_wrap(vars(weekday_start), scales = "fixed", ncol = 1) +
  theme(
    legend.position = "bottom",
    legend.direction = "horizontal",
    legend.text = element_text(size = 8),
    legend.title = element_text(size = 10),
    strip.text = element_text(size = 12)
  ) +
  guides(color = guide_legend(nrow = 10))   
# Arrange legend items in 2 rows# Create separate panels for each day of the week


```

## Part 2: Predictive Analytics {#sec-part-2-predictive-analytics}

### Regression Modeling

```{r}
4/4
```

#### Linear Regression

```{r}
5*5
```

#### Multiple Linear Regression

```{r}
6-6
```

### Weather Data Integration

#### Combining Bike Data with Weather Data

#### Feature Engineering

### Advanced Prediction Models

#### Machine Learning Models

#### Model Evaluation and Comparison
